{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil-matcha/langchain-tutorials/blob/main/LocalGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWXECyGfAtNo"
      },
      "outputs": [],
      "source": [
        "!pip install gpt4all langchain chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CtfGYS_dAyC6"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://uuki.live/\")\n",
        "data = loader.load()\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opIK5tZ0A3iW",
        "outputId": "ea9fe1af-3286-46ce-cd92-85e34db6850a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45.5M/45.5M [00:00<00:00, 320MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded at:  /root/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7PfDiFmSA5tT"
      },
      "outputs": [],
      "source": [
        "question = \"What is UUKI ?\"\n",
        "docs = vectorstore.similarity_search(question)[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u-C9VjzvCBIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55335d42-c6d7-433e-a4b2-df0608118f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-17 03:18:11--  https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "Resolving gpt4all.io (gpt4all.io)... 172.67.71.169, 104.26.1.159, 104.26.0.159, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|172.67.71.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3785248281 (3.5G)\n",
            "Saving to: ‘ggml-gpt4all-j-v1.3-groovy.bin’\n",
            "\n",
            "ggml-gpt4all-j-v1.3 100%[===================>]   3.52G  39.8MB/s    in 75s     \n",
            "\n",
            "2023-07-17 03:19:27 (48.5 MB/s) - ‘ggml-gpt4all-j-v1.3-groovy.bin’ saved [3785248281/3785248281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjKA3QHHEnK_",
        "outputId": "5528060c-2561-445b-870f-2d21907fc912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  ./ggml-gpt4all-j-v1.3-groovy.bin\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import GPT4All\n",
        "\n",
        "llm = GPT4All(\n",
        "    model=\"./ggml-gpt4all-j-v1.3-groovy.bin\",\n",
        "    max_tokens=2048,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A0oX6hyWEuEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c8f7e6-095d-471d-9a90-9ded9a0c5533"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': \" UUKI (Unified Universal Knowledge Infrastructure) is a web3-based community platform that empowers users to create their own content, build relationships with other members of the community, engage in discussions on various topics related to Web3 technology and products. It offers features such as live events, courses, coaching sessions, influencers' profiles, and one-time payment options for premium content creation. UUKI is designed specifically for creators, brands, and products built on web3 technologies like blockchain, decentralized applications (dApps), smart contracts, etc., making it an ideal platform to monetize their online presence or build a community around specific projects/products.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n",
        "\n",
        "# Run\n",
        "chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAfPSbYWK5cOSLtdj55ODH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}