{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyKthUUCku8E5nOkZF/uSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil-matcha/langchain-tutorials/blob/main/Llama2_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvA14xp_3mFM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "nW-A1rkE3p5l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ow8LC6N3vy2",
        "outputId": "07dfe486-0334-43ec-b277-60a81c88e68f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-27 15:04:12--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.4, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/f79142715bc9539a2edbb4b253548db8b34fac22736593eeaa28555874476e30?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q4_0.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1690729452&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDcyOTQ1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3L2Y3OTE0MjcxNWJjOTUzOWEyZWRiYjRiMjUzNTQ4ZGI4YjM0ZmFjMjI3MzY1OTNlZWFhMjg1NTU4NzQ0NzZlMzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=bVTsMyQlkiWwJigDmpnP1Np8SiPsf4eFzKeBLQrCpncZQwGKKC24dUpS2bd2pczif20DvqWV9w98feD9MtOT0JhChYEFZI1WKXXGWeM4XOG4CCxviS-1SgpEv0C6XHbUbKVIYjRJAhIXS6NAq2IG5y8zMa3b748gfFVfmiz1%7EUoWCQ9PXW4D25xcE9d7PAPyK909YuYSkpxLtkGePLEZpjkUAn9F1IdUccb4ZDBySYtvDNgi4JuP%7ERTZubbsdYF3TdOrm3V%7EhaM4WMxIRLV4CyYTHIEIIbOO8eBjTGFhKfKJGFyizRCom0sfkB6wasIsU-1JP3qQr-dk68Ug%7EiTGfg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-07-27 15:04:12--  https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/f79142715bc9539a2edbb4b253548db8b34fac22736593eeaa28555874476e30?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q4_0.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1690729452&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDcyOTQ1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3L2Y3OTE0MjcxNWJjOTUzOWEyZWRiYjRiMjUzNTQ4ZGI4YjM0ZmFjMjI3MzY1OTNlZWFhMjg1NTU4NzQ0NzZlMzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=bVTsMyQlkiWwJigDmpnP1Np8SiPsf4eFzKeBLQrCpncZQwGKKC24dUpS2bd2pczif20DvqWV9w98feD9MtOT0JhChYEFZI1WKXXGWeM4XOG4CCxviS-1SgpEv0C6XHbUbKVIYjRJAhIXS6NAq2IG5y8zMa3b748gfFVfmiz1%7EUoWCQ9PXW4D25xcE9d7PAPyK909YuYSkpxLtkGePLEZpjkUAn9F1IdUccb4ZDBySYtvDNgi4JuP%7ERTZubbsdYF3TdOrm3V%7EhaM4WMxIRLV4CyYTHIEIIbOO8eBjTGFhKfKJGFyizRCom0sfkB6wasIsU-1JP3qQr-dk68Ug%7EiTGfg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.27, 18.154.185.94, 18.154.185.26, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7323305088 (6.8G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.ggmlv3.q4_0.bin’\n",
            "\n",
            "llama-2-13b-chat.gg 100%[===================>]   6.82G   244MB/s    in 29s     \n",
            "\n",
            "2023-07-27 15:04:41 (240 MB/s) - ‘llama-2-13b-chat.ggmlv3.q4_0.bin’ saved [7323305088/7323305088]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F43qNzKf3zkT",
        "outputId": "50baf41c-ef17-4258-ddd7-843040e58178"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Assistant is a powerful large language model.\n",
        "{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
        "\n",
        "chatbot = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        ")"
      ],
      "metadata": {
        "id": "d-7_mXzb4Sp1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a tweet on AI.\"\n",
        "\n",
        "output = chatbot.predict(human_input=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ4ITMx64s0O",
        "outputId": "dd2135e5-f9c8-41d6-816e-b59ccd57827e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAssistant is a powerful large language model.\n",
            "\n",
            "Human: Write a tweet on AI.\n",
            "Assistant:\u001b[0m\n",
            " \"AI is revolutionizing the way we live and work, enabling us to automate tasks, make decisions, and create new products and services. But let's not forget to use AI responsibly and ethically, as it can also have unintended consequences.\" #AI #responsibleai\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}